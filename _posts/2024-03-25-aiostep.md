---
layout: post
title: AI-O Step by Step
date: 2024-03-25 01:17:13 +0900
categories: [AI]
tags: [AI, RVC, SD]
description: RVC, Riffusion, Stable Diffusion 가이드
---

> RVC, Riffusion, Stable Diffusion 가이드


* * *

RVC (Retrieval-based Voice Conversion)
--------------------------------------

> Retrieval-based Voice Conversion  
> 검색 기반 음성 변환  
>   
> AI 음성 합성 기술으로 기존의 Diff-SVC와 비슷한 형태이지만 Diff-SVC는 [Stable Diffusion](https://namu.wiki/w/Stable%20Diffusion)을 이용해 음파 이미지를 만드는 방식이고 RVC는 기존의 음성데이터를 이용해 변조를 하는 방식이다. 음성 변조와 비슷하다고 생각하면 될 듯하다.  
>   
> _\- 나무위키_

* * *

### Local에서 RVC 사용하기

RVC Github에서 문서를 읽으며 따라하는 것을 추천합니다.  
가이드와 다른 내용은 Github 문서를 기준으로 따라해주세요.

Local에서 RVC를 쓰려면 2가지 방법이 있습니다.

1\. Git 저장소를 Clone 해서 직접 사용  
2\. 이미 정리 된 파일을 다운받아 사용

두 방법 모두 간단하게 짚고 넘어가겠습니다.

#### Git 저장소를 Clone 해서 직접 사용

[RVC-Project/Retrieval-based-Voice-Conversion-WebUI](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/kr/README.ko.md)

위의 Github에 있는 한글 가이드를 보고 따라하면 됩니다.

#### 이미 정리 된 파일을 다운받아 사용

[lj1995/VoiceConversionWebUI](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main)

직접 따라하는 것은 의존성 문제도, 시간 문제도 있기 때문에 다운로드를 추천합니다.  
위 Huggingface에서 [`RVC-beta.7z`](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/RVC-beta.7z?ref=saykcay.dev) 혹은 대충 최신 버전의 파일을 다운로드합니다.

이후 압축을 풀고, `go-web.bat`을 실행합니다.

* * *

### UVR로 노래에서 Vocal, MR 분리하기

Ultimate Vocal Remover는 AI 기술을 이용하여,  
노래에서 MR과 Vocal을 추출하는 프로그램입니다. 쉽고, 결과물이 제법 잘 나오는 듯 합니다.

프로그램 설치는 아래의 공식 Github Release에서 각 배포판에 맞는 파일을 설치합니다.

[Anjok07/ultimatevocalremovergui](https://github.com/Anjok07/ultimatevocalremovergui/releases)

![](/assets/post/aiostep/uvr-screen.png)

프로그램을 실행하면, 위와 같은 화면이 나옵니다.  
`Select Input`과 `Select Output`을 클릭하여 노래와 출력물이 나올 폴더를 각각 지정한 뒤,  
`Start Processing`을 진행하면 `Output`에 `Vocals`와 `Instrumental`가 생성됩니다.  
이 `Vocals`가 깔끔할 수록 더 좋은 결과가 나옵니다.

`Vocals`는 모델을 학습할 때 쓸 수도 있고, `VC Client`에 넣어서 커버를 할 때 쓸 수 있습니다.

* * *

### 모델 학습 가이드

> 다음의 글을 참조하여 작성되었습니다.  
> [AI로 버튜버 목소리 커버 만들기 - 키즈나 아이 마이너 갤러리](https://gall.dcinside.com/mgallery/board/view/?id=kizunaai&no=7984215)
{: .prompt-tip }

학습할 `Vocals` 파일은 8초에서 15초 정도로, 총 10분에서 15분 정도의 파일이 있으면 좋습니다.

이후 파일을 압축하여 `ZIP` 파일로 만들어둡니다.

![](/assets/post/aiostep/rvc-webui.1.png)

이후 WebUI에서 **`Train 탭`**을 눌러줍니다.

![](/assets/post/aiostep/rvc-webui.2.png)

**모델 이름**은 아무렇게나 정해주시고,  
**dataset 경로**는 `/path/to/dataset.zip` (데이터셋 파일의 경로)  
**training epoch**은 `120 ~ 150`이 적당합니다.  
`是(yes)` 혹은 `否(no)` 아무거나 선택합니다.

이후 `one-click training.` 을 누르면 됩니다.  
Colab기준, 1시간 이상 걸립니다.

아까 실행한 Run RVC GUI 항목 아래에서  
`"epoch <설정한 epoch값> complete..."` 떠있고 변화가 없으면 완료입니다.

#### 목소리 입히기

![](/assets/post/aiostep/rvc-webui.3.png)

모델명이 있다면 그대로 진행하시고, 없다면 weights 폴더에 만들어진 pth 파일을 넣습니다.

아무튼 **모델이름**을 골라주고,

**transpose**는  
남자 음원 -> 높은 여자 목소리 = `+12`  
여자 음원 -> 낮은 남자 목소리 = `-12`  
여자 음원 -> 여자 모델 = `0` 으로 설정합니다.

**pm/harvest**는 `harvest`  
대체적으로 `harvest`, 저음 모델이나 저음 음원에는 무조건 `harvest`

**feature 경로**는,  
**`Retrieval-based-Voice-Conversion-WebUI`**의 `**logs**` 폴더 내에 `(모델명) 폴더` 가 있고,  
폴더 안에 **`total_fea.npy`**라는 파일의 경로를 입력합니다.

![](/assets/post/aiostep/rvc-webui.4.png)

원본 `Vocals` 음원의 위치를 입력합니다.

이후, **`conversion`** 하고 생성된 음원 파일을 저장하면 끝입니다.  
생성된 음원 파일을 MR과 합치면, AI 커버곡 완성입니다.

* * *

### VC Client

[w-okada/voice-changer](https://github.com/w-okada/voice-changer/blob/master/README_ko.md)

위 Github에 들어가서 적절하게 다운로드 받거나,  
[MMVCServerSIO\_win\_onnxgpu-cuda\_v.1.5.3.18a.zip](https://huggingface.co/wok000/vcclient000/blob/main/MMVCServerSIO_win_onnxgpu-cuda_v.1.5.3.18a.zip)를 다운로드 합니다.

압축을 풀고, **`MMVCServerSIO`**를 실행하면, 알아서 다운받고 켜집니다.

![](/assets/post/aiostep/rvc-client.png)

이제, `edit`에서 모델을 추가하여 사용합니다.

* * *

Riffusion
---------

> Riffusion은 오디오가 아닌 사운드 이미지를 사용하여 음악을 생성하는 Seth Forsgren과 Hayk Martiros가 설계한 신경망입니다.  
>   
> _\- 위키백과_

* * *

### Riffusion 사용하기 - Huggingface

[Riffusion • Spectrogram To Music](https://huggingface.co/spaces/fffiloni/spectrogram-to-music)

### Riffusion 사용하기 - 공식 웹페이지

[Riffusion](https://www.riffusion.com/)

* * *

Stable Diffusion
----------------

> 스테이블 디퓨전은 2022년에 출시된 딥 러닝, 텍스트-이미지 모델이다. 텍스트 설명에 따라 상세한 이미지를 생성하는 데 주로 사용되지만 인페인팅, 아웃페인팅, 이미지 생성과 같은 다른 작업에도 적용할 수 있다.  
>   
> _\- 위키백과_

* * *

### Local에서 SDWebUI 사용하기

> 다음의 글을 참조하여 작성되었습니다.  
> [누구나 따라할 수 있는 로컬 자동좌 WebUI 클린 설치 (v1.6.0 기준 수정)](https://arca.live/b/aiart/79413719?ref=saykcay.dev)
{: .prompt-tip }

```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
Invoke-RestMethod -Uri https://get.scoop.sh | Invoke-Expression
```

윈도우 `Powershell` 에 다음과 같이 입력 후, `Python`, `Git` 과 같은 프로그램을 설치하겠습니다.

```powershell
scoop install git
scoop install aria2
scoop install python
```

이제 Stable Diffusion WebUI를 설치합니다.

```powershell
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
```

이제 폴더에 들어가서 `webui-user.bat` 우클릭, 편집합니다.

![](/assets/post/aiostep/sd-webui.1.png)
![](/assets/post/aiostep/sd-webui.2.png)

```bat
@echo off

set PYTHON=
set GIT=
set VENV_DIR=
set COMMANDLINE_ARGS= --xformers --xformers-flash-attention --opt-channelslast --enable-insecure-extension-access
    
call webui.bat
```

내용을 위와 같이 수정합니다.

이후, `webui-user.bat`을 실행합니다.
